{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a79b912",
   "metadata": {
    "papermill": {
     "duration": 0.005875,
     "end_time": "2026-02-13T15:00:15.792951",
     "exception": false,
     "start_time": "2026-02-13T15:00:15.787076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook produces visualizations of the model interpretable parameters (or parameter–derived quantities) in space and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2613a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f85c85a9690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714525f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:15.805404Z",
     "iopub.status.busy": "2026-02-13T15:00:15.805228Z",
     "iopub.status.idle": "2026-02-13T15:00:24.161934Z",
     "shell.execute_reply": "2026-02-13T15:00:24.161045Z"
    },
    "papermill": {
     "duration": 8.363766,
     "end_time": "2026-02-13T15:00:24.162925",
     "exception": false,
     "start_time": "2026-02-13T15:00:15.799159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import dask\n",
    "import diffrax as dfx\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrd\n",
    "from jaxtyping import Float, Key, Real\n",
    "from hydra_zen import instantiate, load_from_yaml\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "from pastax.gridded import Gridded\n",
    "from pastax.simulator import DeterministicSimulator\n",
    "from pastax.trajectory import Trajectory\n",
    "\n",
    "from src.ec_mlp.data_driven_model import DataDrivenModel as StochDataDrivenModel\n",
    "from src.ec_mlp.drift_model import DriftModel as StochDriftModel\n",
    "from src.ec_mlp.trainer_module import TrainerModule as StochTrainerModule\n",
    "from src.ls_mlp.data_driven_model import DataDrivenModel as DeterDataDrivenModel\n",
    "from src.ls_mlp.drift_model import DriftModel as DeterStochModel\n",
    "from src.ls_mlp.trainer_module import TrainerModule as DeterTrainerModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87c0a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:24.174837Z",
     "iopub.status.busy": "2026-02-13T15:00:24.174476Z",
     "iopub.status.idle": "2026-02-13T15:00:24.177281Z",
     "shell.execute_reply": "2026-02-13T15:00:24.176717Z"
    },
    "papermill": {
     "duration": 0.009109,
     "end_time": "2026-02-13T15:00:24.177724",
     "exception": false,
     "start_time": "2026-02-13T15:00:24.168615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT = Path(\".\")\n",
    "\n",
    "STOCH_RUNS_DIR = ROOT / \"euler_criterion/mlp/multirun/2026-02-08/09-28-33\"\n",
    "STOCH_RUN_ID = \"15\"\n",
    "\n",
    "DETER_RUNS_DIR = ROOT / \"least_squares/mlp/multirun/2026-02-05/11-31-57\"\n",
    "DETER_RUN_ID = \"25\"\n",
    "\n",
    "DATA_DIR = ROOT / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d50b3",
   "metadata": {},
   "source": [
    "## Load best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebd952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:24.189275Z",
     "iopub.status.busy": "2026-02-13T15:00:24.189129Z",
     "iopub.status.idle": "2026-02-13T15:00:24.206292Z",
     "shell.execute_reply": "2026-02-13T15:00:24.205640Z"
    },
    "papermill": {
     "duration": 0.023261,
     "end_time": "2026-02-13T15:00:24.206856",
     "exception": false,
     "start_time": "2026-02-13T15:00:24.183595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stoch_run_cfg = load_from_yaml(STOCH_RUNS_DIR / STOCH_RUN_ID / \".hydra/config.yaml\")\n",
    "deter_run_cfg = load_from_yaml(DETER_RUNS_DIR / DETER_RUN_ID / \".hydra/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284da9bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:24.217613Z",
     "iopub.status.busy": "2026-02-13T15:00:24.217466Z",
     "iopub.status.idle": "2026-02-13T15:00:28.281187Z",
     "shell.execute_reply": "2026-02-13T15:00:28.280351Z"
    },
    "papermill": {
     "duration": 4.069915,
     "end_time": "2026-02-13T15:00:28.281852",
     "exception": false,
     "start_time": "2026-02-13T15:00:24.211937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trunk = instantiate(stoch_run_cfg.trunk)()\n",
    "physical_head = instantiate(stoch_run_cfg.physical_head)()\n",
    "mdn_head = instantiate(stoch_run_cfg.mdn_head)()\n",
    "\n",
    "stoch_data_driven_model = StochDataDrivenModel(trunk, physical_head, mdn_head)\n",
    "stoch_drift_model = StochDriftModel(\n",
    "    data_driven_model=stoch_data_driven_model, \n",
    "    stress_normalization=1.0, \n",
    "    wind_normalization=1.0,\n",
    "    delta_t=1.0 * 60.0 * 60.0  # 1 hour in seconds\n",
    ")\n",
    "\n",
    "stoch_drift_model = StochTrainerModule.load_from_checkpoint(\n",
    "    STOCH_RUNS_DIR / STOCH_RUN_ID / \"best_model.ckpt\", drift_model=stoch_drift_model\n",
    ").drift_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8d27f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:28.513249Z",
     "iopub.status.busy": "2026-02-13T15:00:28.513088Z",
     "iopub.status.idle": "2026-02-13T15:00:28.701154Z",
     "shell.execute_reply": "2026-02-13T15:00:28.700422Z"
    },
    "papermill": {
     "duration": 0.194298,
     "end_time": "2026-02-13T15:00:28.701656",
     "exception": false,
     "start_time": "2026-02-13T15:00:28.507358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bertrava/.local/share/mamba/envs/pastax_global_calibration/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:96: The state dict in PosixPath('euler_criterion/mlp/multirun/2026-02-08/09-28-33/15/best_model.ckpt') contains no parameters.\n"
     ]
    }
   ],
   "source": [
    "deter_data_driven_model = DeterDataDrivenModel(trunk, physical_head, mdn_head)\n",
    "deter_drift_model = DeterStochModel(\n",
    "    data_driven_model=deter_data_driven_model, \n",
    "    stress_normalization=1.0, \n",
    "    wind_normalization=1.0\n",
    ")\n",
    "\n",
    "deter_drift_model = DeterTrainerModule.load_from_checkpoint(\n",
    "    DETER_RUNS_DIR / DETER_RUN_ID / \"best_model.ckpt\", drift_model=deter_drift_model\n",
    ").drift_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c82e251",
   "metadata": {},
   "source": [
    "## Reference trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77a3384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:28.713210Z",
     "iopub.status.busy": "2026-02-13T15:00:28.713049Z",
     "iopub.status.idle": "2026-02-13T15:00:28.715619Z",
     "shell.execute_reply": "2026-02-13T15:00:28.714925Z"
    },
    "papermill": {
     "duration": 0.008826,
     "end_time": "2026-02-13T15:00:28.716058",
     "exception": false,
     "start_time": "2026-02-13T15:00:28.707232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from_datetime_str = \"1994-06-01\"\n",
    "to_datetime_str = \"2025-08-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ba637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:28.727324Z",
     "iopub.status.busy": "2026-02-13T15:00:28.727179Z",
     "iopub.status.idle": "2026-02-13T15:00:29.451404Z",
     "shell.execute_reply": "2026-02-13T15:00:29.450659Z"
    },
    "papermill": {
     "duration": 0.730886,
     "end_time": "2026-02-13T15:00:29.452257",
     "exception": false,
     "start_time": "2026-02-13T15:00:28.721371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "traj_ds = xr.open_zarr(DATA_DIR / f\"gdp_interp_clean_{from_datetime_str}_{to_datetime_str}_test_traj.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10a06d",
   "metadata": {},
   "source": [
    "## Forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c29390",
   "metadata": {},
   "outputs": [],
   "source": [
    "duacs_ds = xr.open_zarr(DATA_DIR / f\".zarr\")\n",
    "era5_ds = xr.open_zarr(DATA_DIR / f\".zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a8c3b",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(TorchDataset):\n",
    "    def __init__(self, traj_ds: xr.Dataset, duacs_ds: xr.Dataset, era5_ds: xr.Dataset, periodic_domain: bool = True):\n",
    "        self.traj_ds = traj_ds\n",
    "        self.duacs_ds = duacs_ds\n",
    "        self.era5_ds = era5_ds\n",
    "        self.periodic_domain = periodic_domain\n",
    "\n",
    "        max_travel_distance = .5  # in ° / day ; inferred from data\n",
    "        traj_t0_t1 = traj_ds.time.isel(traj=0)[np.asarray([0, -1])]\n",
    "        n_days = ((traj_t0_t1[-1] - traj_t0_t1[0]) / np.timedelta64(1, \"D\")).astype(int).item()\n",
    "        self.max_travel_distance = max_travel_distance * n_days\n",
    "\n",
    "        self.duacs_nt = duacs_ds.time.size\n",
    "        self.duacs_mint = duacs_ds.time.min()\n",
    "        self.duacs_dt = duacs_ds.time[1] - duacs_ds.time[0]\n",
    "        self.duacs_t_di = np.ceil(np.timedelta64(n_days, \"D\") / self.duacs_dt)\n",
    "        self.duacs_nlat = duacs_ds.latitude.size\n",
    "        self.duacs_minlat = duacs_ds.latitude.min()\n",
    "        self.duacs_dlat = duacs_ds.latitude[1] - duacs_ds.latitude[0]  # regular grid\n",
    "        self.duacs_lat_di = np.ceil(self.max_travel_distance / self.duacs_dlat)\n",
    "        self.duacs_nlon = duacs_ds.longitude.size\n",
    "        self.duacs_minlon = duacs_ds.longitude.min()\n",
    "        self.duacs_dlon = duacs_ds.longitude[1] - duacs_ds.longitude[0]  # regular grid\n",
    "        self.duacs_lon_di = np.ceil(self.max_travel_distance / self.duacs_dlon)\n",
    "\n",
    "        self.era5_nt = era5_ds.time.size\n",
    "        self.era5_mint = era5_ds.time.min()\n",
    "        self.era5_dt = era5_ds.time[1] - era5_ds.time[0]\n",
    "        self.era5_t_di = np.ceil(np.timedelta64(n_days, \"D\") / self.era5_dt)\n",
    "        self.era5_nlat = era5_ds.latitude.size\n",
    "        self.era5_minlat = era5_ds.latitude.min()\n",
    "        self.era5_dlat = era5_ds.latitude[1] - era5_ds.latitude[0]  # regular grid\n",
    "        self.era5_lat_di = np.ceil(self.max_travel_distance / self.era5_dlat)\n",
    "        self.era5_nlon = era5_ds.longitude.size\n",
    "        self.era5_minlon = era5_ds.longitude.min()\n",
    "        self.era5_dlon = era5_ds.longitude[1] - era5_ds.longitude[0]  # regular grid\n",
    "        self.era5_lon_di = np.ceil(self.max_travel_distance / self.era5_dlon)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.traj_ds.traj.size\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx: int\n",
    "    ) -> tuple[\n",
    "        tuple[\n",
    "            Float[np.ndarray, \"traj_length\"], \n",
    "            Float[np.ndarray, \"traj_length\"], \n",
    "            Float[np.ndarray, \"traj_length\"], \n",
    "            Float[np.ndarray, \"traj_length\"]\n",
    "        ],\n",
    "        tuple[dict[str, Float[np.ndarray, \"T N N\"]], \n",
    "            Float[np.ndarray, \"T\"], \n",
    "            Float[np.ndarray, \"N\"], \n",
    "            Float[np.ndarray, \"N\"]\n",
    "        ],\n",
    "        tuple[dict[str, Float[np.ndarray, \"T N N\"]], \n",
    "            Float[np.ndarray, \"T\"], \n",
    "            Float[np.ndarray, \"N\"], \n",
    "            Float[np.ndarray, \"N\"]\n",
    "        ]\n",
    "    ]:\n",
    "        traj_arrays = self.__get_traj_arrays(idx)\n",
    "        duacs_arrays = self.__get_duacs_arrays(*traj_arrays[:3])\n",
    "        era5_arrays = self.__get_era5_arrays(*traj_arrays[:3])\n",
    "        \n",
    "        return traj_arrays, duacs_arrays, era5_arrays\n",
    "    \n",
    "    def __get_traj_arrays(\n",
    "        self, idx: int\n",
    "    ) -> tuple[\n",
    "        Float[np.ndarray, \"traj_length\"], \n",
    "        Float[np.ndarray, \"traj_length\"], \n",
    "        Float[np.ndarray, \"traj_length\"], \n",
    "        Float[np.ndarray, \"traj_length\"]\n",
    "    ]:\n",
    "        traj_subset = self.traj_ds.isel(traj=idx)\n",
    "        \n",
    "        traj_lat = traj_subset.lat.values.ravel()\n",
    "        traj_lon = traj_subset.lon.values.ravel()\n",
    "        traj_time = traj_subset.time.values.ravel().astype(\"datetime64[s]\").astype(int)  # in seconds\n",
    "        traj_id = traj_subset.id.values.ravel()\n",
    "        \n",
    "        return traj_lat, traj_lon, traj_time, traj_id\n",
    "    \n",
    "    def __get_duacs_arrays(\n",
    "        self, \n",
    "        traj_lat: Float[np.ndarray, \"traj_length\"], \n",
    "        traj_lon: Float[np.ndarray, \"traj_length\"], \n",
    "        traj_time: Float[np.ndarray, \"traj_length\"]\n",
    "    ) -> tuple[\n",
    "        dict[str, Float[np.ndarray, \"T N N\"]], \n",
    "        Float[np.ndarray, \"T\"], \n",
    "        Float[np.ndarray, \"N\"], \n",
    "        Float[np.ndarray, \"N\"]\n",
    "    ]:\n",
    "        return self.__get_forcing_arrays(\n",
    "            traj_lat, traj_lon, traj_time,\n",
    "            self.duacs_ds, (\"ugos\", \"vgos\"),\n",
    "            self.duacs_nt, self.duacs_mint, self.duacs_dt, self.duacs_t_di,\n",
    "            self.duacs_nlat, self.duacs_minlat, self.duacs_dlat, self.duacs_lat_di,\n",
    "            self.duacs_nlon, self.duacs_minlon, self.duacs_dlon, self.duacs_lon_di\n",
    "        )\n",
    "    \n",
    "    def __get_era5_arrays(\n",
    "        self, \n",
    "        traj_lat: Float[np.ndarray, \"traj_length\"], \n",
    "        traj_lon: Float[np.ndarray, \"traj_length\"], \n",
    "        traj_time: Float[np.ndarray, \"traj_length\"]\n",
    "    ) -> tuple[\n",
    "        dict[str, Float[np.ndarray, \"T N N\"]], \n",
    "        Float[np.ndarray, \"T\"], \n",
    "        Float[np.ndarray, \"N\"], \n",
    "        Float[np.ndarray, \"N\"]\n",
    "    ]:\n",
    "        return self.__get_forcing_arrays(\n",
    "            traj_lat, traj_lon, traj_time,\n",
    "            self.era5_ds, (\"eastward_stress\", \"northward_stress\", \"eastward_wind\", \"northward_wind\"),\n",
    "            self.era5_nt, self.era5_mint, self.era5_dt, self.era5_t_di,\n",
    "            self.era5_nlat, self.era5_minlat, self.era5_dlat, self.era5_lat_di,\n",
    "            self.era5_nlon, self.era5_minlon, self.era5_dlon, self.era5_lon_di\n",
    "        )\n",
    "    \n",
    "    def __get_forcing_arrays(\n",
    "        self, \n",
    "        traj_lat: Float[np.ndarray, \"traj_length\"], \n",
    "        traj_lon: Float[np.ndarray, \"traj_length\"], \n",
    "        traj_time: Float[np.ndarray, \"traj_length\"],\n",
    "        ds: xr.Dataset, \n",
    "        vars_names: tuple[str],\n",
    "        nt: int, mint: np.datetime64, dt: np.timedelta64, t_di: int, \n",
    "        nlat: int, minlat: float, dlat: float, lat_di: float, \n",
    "        nlon: int, minlon: float, dlon: float, lon_di: float\n",
    "    ) -> tuple[\n",
    "        dict[str, Float[np.ndarray, \"T N N\"]], \n",
    "        Float[np.ndarray, \"T\"], \n",
    "        Float[np.ndarray, \"N\"], \n",
    "        Float[np.ndarray, \"N\"]\n",
    "    ]:\n",
    "        def get_latlon_minmax(latlon0_i, latlon_di):\n",
    "            min_i = (latlon0_i - latlon_di).astype(int).item()\n",
    "            max_i = (latlon0_i + latlon_di).astype(int).item()\n",
    "            return min_i, max_i\n",
    "        \n",
    "        def get_pads(min_i, max_i, n):\n",
    "            padleft = max(0, -min_i)\n",
    "            min_i = max(0, min_i)\n",
    "            padright = max(0, max_i - (n - 1))\n",
    "            max_i = min(n - 1, max_i)\n",
    "            return (padleft, padright), (min_i, max_i)\n",
    "    \n",
    "        t0 = traj_time[0].astype(\"datetime64[s]\")\n",
    "        lat0 = traj_lat[0]\n",
    "        lon0 = traj_lon[0]\n",
    "\n",
    "        t0_i = np.floor((t0 - mint) / dt)\n",
    "        lat0_i = ((lat0 - minlat) / dlat).round()\n",
    "        lon0_i = ((lon0 - minlon) / dlon).round()\n",
    "\n",
    "        tmin_i = t0_i.astype(int).item()\n",
    "        tmax_i = (t0_i + t_di).astype(int).item()\n",
    "        latmin_i, latmax_i = get_latlon_minmax(lat0_i, lat_di)\n",
    "        lonmin_i, lonmax_i = get_latlon_minmax(lon0_i, lon_di)\n",
    "\n",
    "        (t_padleft, t_padright), (tmin_i, tmax_i) = get_pads(tmin_i, tmax_i, nt)\n",
    "        (lat_padleft, lat_padright), (latmin_i, latmax_i) = get_pads(latmin_i, latmax_i, nlat)\n",
    "        (lon_padleft, lon_padright), (lonmin_i, lonmax_i) = get_pads(lonmin_i, lonmax_i, nlon)\n",
    "\n",
    "        patch = ds.isel(\n",
    "            time=slice(tmin_i, tmax_i + 1),\n",
    "            latitude=slice(latmin_i, latmax_i + 1), \n",
    "            longitude=slice(lonmin_i, lonmax_i + 1)\n",
    "        )\n",
    "\n",
    "        patch_vars = dict((var_name, patch[var_name]) for var_name in vars_names)\n",
    "        patch_time = patch.time.astype(\"datetime64[s]\").astype(int)  # in seconds\n",
    "        patch_lat = patch.latitude\n",
    "        patch_lon = patch.longitude\n",
    "\n",
    "        if self.periodic_domain:  # periodic global domain\n",
    "            if lon_padleft != 0:\n",
    "                patch_left = ds.isel(\n",
    "                    time=slice(tmin_i, tmax_i + 1),\n",
    "                    latitude=slice(latmin_i, latmax_i + 1), \n",
    "                    longitude=slice(nlon - lon_padleft, nlon)  # right part goes to the left\n",
    "                )\n",
    "\n",
    "                patch_vars_left = dict((var, patch_left[var]) for var in vars_names)\n",
    "                lon_left = patch_left.longitude\n",
    "\n",
    "                patch_vars = dict(\n",
    "                    (var_name, np.concat([patch_vars_left[var_name], patch_vars[var_name]], axis=-1)) \n",
    "                    for var_name in vars_names\n",
    "                )\n",
    "                patch_lon = np.concat([lon_left, patch_lon])\n",
    "\n",
    "                lon_padleft = 0\n",
    "\n",
    "            if lon_padright != 0:\n",
    "                patch_right = ds.isel(\n",
    "                    time=slice(tmin_i, tmax_i + 1),\n",
    "                    latitude=slice(latmin_i, latmax_i + 1), \n",
    "                    longitude=slice(0, lon_padright)  # left part goes to the right\n",
    "                )\n",
    "\n",
    "                patch_vars_right = [patch_right[var] for var in vars_names]\n",
    "                lon_right = patch_right.longitude\n",
    "\n",
    "                patch_vars = dict(\n",
    "                    (var_name, np.concat([patch_vars[var_name], patch_vars_right[var_name]], axis=-1)) \n",
    "                    for var_name in vars_names\n",
    "                )\n",
    "                patch_lon = np.concat([patch_lon, lon_right])\n",
    "\n",
    "                lon_padright = 0\n",
    "\n",
    "        patch_vars = dict(\n",
    "            (\n",
    "                var_name,\n",
    "                np.pad(\n",
    "                    patch_vars[var_name], \n",
    "                    ((t_padleft, t_padright), (lat_padleft, lat_padright), (lon_padleft, lon_padright)), \n",
    "                    mode=\"edge\"\n",
    "                )\n",
    "            ) for var_name in vars_names\n",
    "        )\n",
    "        patch_time = np.pad(patch_time, (t_padleft, t_padright), mode=\"edge\")\n",
    "        patch_lat = np.pad(patch_lat, (lat_padleft, lat_padright), mode=\"edge\")\n",
    "        patch_lon = np.pad(patch_lon, (lon_padleft, lon_padright), mode=\"edge\")\n",
    "        \n",
    "        return patch_vars, patch_time, patch_lat, patch_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34427",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = Dataset(traj_ds, duacs_ds, era5_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c20a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:29.463951Z",
     "iopub.status.busy": "2026-02-13T15:00:29.463653Z",
     "iopub.status.idle": "2026-02-13T15:00:29.466252Z",
     "shell.execute_reply": "2026-02-13T15:00:29.465649Z"
    },
    "papermill": {
     "duration": 0.008893,
     "end_time": "2026-02-13T15:00:29.466711",
     "exception": false,
     "start_time": "2026-02-13T15:00:29.457818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "dl_num_workers = 32\n",
    "multiprocessing_context = \"spawn\"\n",
    "prefetch_factor = 2\n",
    "\n",
    "dask_n_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_jax_dataloader = DataLoader(\n",
    "    torch_dataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=dl_num_workers, \n",
    "    pin_memory=False,\n",
    "    multiprocessing_context=multiprocessing_context,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    persistent_workers=True, \n",
    "    in_order=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f45eb3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:29.477889Z",
     "iopub.status.busy": "2026-02-13T15:00:29.477752Z",
     "iopub.status.idle": "2026-02-13T15:00:29.481048Z",
     "shell.execute_reply": "2026-02-13T15:00:29.480490Z"
    },
    "papermill": {
     "duration": 0.009559,
     "end_time": "2026-02-13T15:00:29.481531",
     "exception": false,
     "start_time": "2026-02-13T15:00:29.471972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_jax(arr: Float[np.ndarray, \"...\"]) -> Float[jax.Array, \"...\"]:\n",
    "    return jnp.asarray(arr)\n",
    "\n",
    "\n",
    "@eqx.filter_jit  # this improves performances\n",
    "def to_trajectories(\n",
    "    traj_arrays: tuple[\n",
    "        Float[jax.Array, \"batch traj_length\"], \n",
    "        Float[jax.Array, \"batch traj_length\"], \n",
    "        Float[jax.Array, \"batch traj_length\"], \n",
    "        Float[jax.Array, \"batch traj_length\"]\n",
    "    ]\n",
    ") -> Trajectory:\n",
    "    traj_lat, traj_lon, traj_time, traj_id = traj_arrays\n",
    "\n",
    "    traj_latlon = jnp.stack((traj_lat, traj_lon), axis=-1)\n",
    "    trajectories = eqx.filter_vmap(\n",
    "        lambda _latlon, _time, _id: Trajectory.from_array(values=_latlon, times=_time, id=_id)\n",
    "    )(\n",
    "        traj_latlon, traj_time, traj_id\n",
    "    )\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "@eqx.filter_jit  # this improves performances\n",
    "def to_gridded(\n",
    "    forcings_arrays: tuple[\n",
    "        dict[str, Float[jax.Array, \"batch T N N\"]], \n",
    "        Float[jax.Array, \"batch T\"], \n",
    "        Float[jax.Array, \"batch N\"], \n",
    "        Float[jax.Array, \"batch N\"]\n",
    "    ]\n",
    ") -> Gridded:\n",
    "    dict_vars, time, lat, lon = forcings_arrays\n",
    "    \n",
    "    gridded = eqx.filter_vmap(Gridded.from_array)(\n",
    "        dict_vars, time, lat, lon\n",
    "    )\n",
    "\n",
    "    return gridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e7016",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(scheduler=\"threads\", num_workers=dask_n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451852a",
   "metadata": {},
   "source": [
    "## Drift integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e03197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_from_epoch(seconds):\n",
    "    # Days since Unix epoch (1970-01-01)\n",
    "    days = seconds // 86400\n",
    "\n",
    "    # Convert Unix epoch days to civil date where a year starts on March 1st\n",
    "    # Unix epoch 1970-01-01 corresponds to civil date offset 719468\n",
    "    z = days + 719468\n",
    "\n",
    "    # The leap-year pattern repeats every 400 years (146097 days), one era corresponds to one such period\n",
    "    era = (z >= 0).astype(jnp.int64) * z // 146097 + (z < 0).astype(jnp.int64) * ((z - 146096) // 146097)\n",
    "    doe = z - era * 146097\n",
    "    yoe = (doe - doe // 1460 + doe // 36524 - doe // 146096) // 365\n",
    "    doy = doe - (365 * yoe + yoe // 4 - yoe // 100)\n",
    "    \n",
    "    # The month is shifted back so that January is month 1\n",
    "    mp = (5 * doy + 2) // 153\n",
    "    month = mp + 3 - 12 * (mp // 10)\n",
    "\n",
    "    return month\n",
    "\n",
    "\n",
    "def stoch_dynamics(\n",
    "    t: Real[jax.Array, \"\"], \n",
    "    x: Float[jax.Array, \"2\"], \n",
    "    args: tuple[StochDriftModel, Float[jax.Array, \"\"], Gridded, Gridded, Real[jax.Array, \"\"], Key[jax.Array, \"n_steps\"]]\n",
    ") -> Float[jax.Array, \"2\"]:\n",
    "    lat, lon = x\n",
    "    drift_model, delta_t, duacs, era5, t0, keys = args\n",
    "\n",
    "    key = keys[((t - t0) / delta_t).astype(int)]\n",
    "\n",
    "    duacs_forcing = duacs.interp(\"ugos\", \"vgos\", time=t, latitude=lat, longitude=lon)\n",
    "    era5_forcing = era5.interp(\n",
    "        \"eastward_stress\", \"northward_stress\", \"eastward_wind\", \"northward_wind\", time=t, latitude=lat, longitude=lon\n",
    "    )\n",
    "\n",
    "    ugos, vgos = duacs_forcing[\"ugos\"], duacs_forcing[\"vgos\"]\n",
    "    eastward_stress, northward_stress = era5_forcing[\"eastward_stress\"], era5_forcing[\"northward_stress\"]\n",
    "    eastward_wind, northward_wind = era5_forcing[\"eastward_wind\"], era5_forcing[\"northward_wind\"]\n",
    "\n",
    "    u, v = drift_model.sample_velocity(\n",
    "        ugos, vgos, eastward_stress, northward_stress, eastward_wind, northward_wind, \n",
    "        month_from_epoch(t), lat, lon,\n",
    "        key, \n",
    "        delta_t\n",
    "    )\n",
    "\n",
    "    return jnp.stack((v, u), axis=-1)\n",
    "\n",
    "\n",
    "def mode_dynamics(\n",
    "    t: Real[jax.Array, \"\"], \n",
    "    x: Float[jax.Array, \"2\"], \n",
    "    args: tuple[StochDriftModel, Gridded, Gridded]\n",
    ") -> Float[jax.Array, \"2\"]:\n",
    "    lat, lon = x\n",
    "    drift_model, duacs, era5 = args\n",
    "\n",
    "    duacs_forcing = duacs.interp(\"ugos\", \"vgos\", time=t, latitude=lat, longitude=lon)\n",
    "    era5_forcing = era5.interp(\n",
    "        \"eastward_stress\", \"northward_stress\", \"eastward_wind\", \"northward_wind\", time=t, latitude=lat, longitude=lon\n",
    "    )\n",
    "\n",
    "    ugos, vgos = duacs_forcing[\"ugos\"], duacs_forcing[\"vgos\"]\n",
    "    eastward_stress, northward_stress = era5_forcing[\"eastward_stress\"], era5_forcing[\"northward_stress\"]\n",
    "    eastward_wind, northward_wind = era5_forcing[\"eastward_wind\"], era5_forcing[\"northward_wind\"]\n",
    "\n",
    "    u, v = drift_model.estimate_mode_velocity(\n",
    "        ugos, vgos, eastward_stress, northward_stress, eastward_wind, northward_wind, \n",
    "        month_from_epoch(t), lat, lon\n",
    "    )\n",
    "\n",
    "    return jnp.stack((v, u), axis=-1)\n",
    "\n",
    "\n",
    "def deter_dynamics(\n",
    "    t: Real[jax.Array, \"\"], \n",
    "    x: Float[jax.Array, \"2\"], \n",
    "    args: tuple[DeterStochModel, Gridded, Gridded]\n",
    ") -> Float[jax.Array, \"2\"]:\n",
    "    lat, lon = x\n",
    "    drift_model, duacs, era5 = args\n",
    "\n",
    "    duacs_forcing = duacs.interp(\"ugos\", \"vgos\", time=t, latitude=lat, longitude=lon)\n",
    "    era5_forcing = era5.interp(\n",
    "        \"eastward_stress\", \"northward_stress\", \"eastward_wind\", \"northward_wind\", time=t, latitude=lat, longitude=lon\n",
    "    )\n",
    "\n",
    "    ugos, vgos = duacs_forcing[\"ugos\"], duacs_forcing[\"vgos\"]\n",
    "    eastward_stress, northward_stress = era5_forcing[\"eastward_stress\"], era5_forcing[\"northward_stress\"]\n",
    "    eastward_wind, northward_wind = era5_forcing[\"eastward_wind\"], era5_forcing[\"northward_wind\"]\n",
    "\n",
    "    uv = drift_model(\n",
    "        ugos, vgos, eastward_stress, northward_stress, eastward_wind, northward_wind, \n",
    "        month_from_epoch(t), lat, lon\n",
    "    )\n",
    "\n",
    "    u = uv.real\n",
    "    v = uv.imag\n",
    "\n",
    "    return jnp.stack((v, u), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = jnp.asarray(15 * 60.0)  # 15 minutes in seconds\n",
    "sim_days = 6\n",
    "n_steps = (sim_days * 24 * 60 * 60 / delta_t).astype(int)\n",
    "\n",
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22806acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = DeterministicSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(\n",
    "    trajectory: Trajectory, \n",
    "    duacs: Gridded, \n",
    "    era5: Gridded, \n",
    "    key: Key[jax.Array, \"\"]\n",
    ") -> tuple[Trajectory, Trajectory, Trajectory]:\n",
    "    x0 = trajectory.origin\n",
    "    ts = trajectory.time.value\n",
    "\n",
    "    solver_cls = dfx.Tsit5\n",
    "    saveat = lambda: dfx.SaveAt(ts=ts)\n",
    "    step_size_controller = lambda: dfx.StepTo(ts[0], ts[-1], n_steps + 1)\n",
    "\n",
    "    sample_keys = jax.random.split(key, n_samples)\n",
    "\n",
    "    stoch_args = (stoch_drift_model, delta_t, duacs, era5, ts[0])\n",
    "    mode_args = (stoch_drift_model, duacs, era5)\n",
    "    deter_args = (deter_drift_model, duacs, era5)\n",
    "\n",
    "    stoch_sol = jax.vmap(\n",
    "        lambda sample_key: simulator(\n",
    "            dynamics=stoch_dynamics,\n",
    "            args=(*stoch_args, jax.random.split(sample_key, n_steps)),\n",
    "            x0=x0,\n",
    "            ts=ts,\n",
    "            solver=solver_cls(),\n",
    "            saveat=saveat(),\n",
    "            stepsize_controller=step_size_controller(),\n",
    "            max_steps=n_steps,\n",
    "        )\n",
    "    )(sample_keys)\n",
    "\n",
    "    mode_sol = simulator(\n",
    "        dynamics=mode_dynamics,\n",
    "        args=mode_args,\n",
    "        x0=x0,\n",
    "        ts=ts,\n",
    "        solver=solver_cls(),\n",
    "        saveat=saveat(),\n",
    "        stepsize_controller=step_size_controller()\n",
    "    )\n",
    "\n",
    "    deter_sol = simulator(\n",
    "        dynamics=deter_dynamics,\n",
    "        args=deter_args,\n",
    "        x0=x0,\n",
    "        ts=ts,\n",
    "        solver=solver_cls(),\n",
    "        saveat=saveat(),\n",
    "        stepsize_controller=step_size_controller()\n",
    "    )\n",
    "\n",
    "    return stoch_sol, mode_sol, deter_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db7237ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:00:29.492610Z",
     "iopub.status.busy": "2026-02-13T15:00:29.492473Z",
     "iopub.status.idle": "2026-02-13T15:00:29.497801Z",
     "shell.execute_reply": "2026-02-13T15:00:29.497232Z"
    },
    "papermill": {
     "duration": 0.011554,
     "end_time": "2026-02-13T15:00:29.498332",
     "exception": false,
     "start_time": "2026-02-13T15:00:29.486778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference_trajectories = []\n",
    "stoch_results = []\n",
    "mode_results = []\n",
    "deter_results = []\n",
    "\n",
    "for trajectory_batch, duacs_batch, era5_batch in tqdm(xr_jax_dataloader):\n",
    "    trajectory_batch = [to_jax(arr) for arr in trajectory_batch]\n",
    "    duacs_batch = [to_jax(arr) for arr in duacs_batch]\n",
    "    era5_batch = [to_jax(arr) for arr in era5_batch]\n",
    "    batch_size = trajectory_batch[0].shape[0]\n",
    "    \n",
    "    trajectory_batch = to_trajectories(trajectory_batch)\n",
    "    duacs_batch = to_gridded(duacs_batch)\n",
    "    era5_batch = to_gridded(era5_batch)\n",
    "    \n",
    "    key, subkey = jrd.split(key, 2)\n",
    "    batch_keys = jrd.split(subkey, batch_size)\n",
    "\n",
    "    batch_results = jax.vmap(simulate)(trajectory_batch, duacs_batch, era5_batch, batch_keys)\n",
    "\n",
    "    reference_trajectories.append(trajectory_batch)\n",
    "    stoch_results.append(batch_results[0])\n",
    "    mode_results.append(batch_results[1])\n",
    "    deter_results.append(batch_results[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.248872,
   "end_time": "2026-02-13T15:00:34.523594",
   "environment_variables": {},
   "exception": null,
   "input_path": "euler_criterion/mlp/analysis/3-test_set_point_errors.ipynb",
   "output_path": "euler_criterion/mlp/analysis/3-test_set_point_errors.ipynb",
   "parameters": {},
   "start_time": "2026-02-13T15:00:14.274722",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
